from langchain.callbacks.base import BaseCallbackHandler
from langchain_community.chat_message_histories import StreamlitChatMessageHistory
from langchain_openai import ChatOpenAI
import streamlit as st
from dotenv import load_dotenv

load_dotenv()

st.set_page_config(
    page_title="Streamlit Agent using streaming",
)

st.subheader("Ask the bot some questions")

class StreamHandler(BaseCallbackHandler):
    """
    StreamHandler manages the streaming of LLM responses in Streamlit.
    
    Streaming allows for real-time display of the AI's response as it's being generated,
    rather than waiting for the entire response to be completed before showing it.
    
    The class uses Streamlit's container to update the UI in real-time.
    """
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        """
        This method is an event handler that's called every time a new token
        is generated by the LLM.
        
        Events in this context refer to specific occurrences during the LLM's
        processing, such as when a new token is generated. The BaseCallbackHandler
        provides methods to handle various events in the LLM pipeline.
        
        Args:
            token (str): The new token generated by the LLM.
            **kwargs: Additional keyword arguments (not used in this implementation).
        """
        self.text += token
        self.container.markdown(self.text)


history = StreamlitChatMessageHistory(key="messages")
if len(history.messages) == 0:
    history.add_ai_message("How can I help you?")

for msg in history.messages:
    st.chat_message(msg.type).write(msg.content)

if prompt := st.chat_input():
    history.add_user_message(prompt)
    st.chat_message("user").write(prompt)
    with st.chat_message("assistant"):
        # Create a StreamHandler to handle the streaming output
        stream_handler = StreamHandler(st.empty())
        
        # Initialize the ChatOpenAI model with streaming enabled and the custom handler
        llm = ChatOpenAI(streaming=True, callbacks=[stream_handler])
        
        # Invoke the model with the chat history, which will stream the response
        response = llm.invoke(history.messages)
        history.add_ai_message(response.content)